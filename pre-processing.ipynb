{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook serves as an exercise to practice all text preprocessing steps on a given text document using 3 different libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load and explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ''\n",
    "with open('data.txt','r') as inputfile:\n",
    "    data = inputfile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuro-linguistic programming (NLP) is a pseudoscientific approach to communication, personal development, and psychotherapy created by Richard Bandler and John Grinder in California, United States in the 1970s. NLP's creators claim there is a connection between neurological processes (neuro-), language (linguistic) and behavioral patterns learned through experience (programming), and that these can be changed to achieve specific goals in life.[1][2] Bandler and Grinder also claim that NLP methodology can \"model\" the skills of exceptional people, allowing anyone to acquire those skills.[3][4] They claim as well that, often in a single session, NLP can treat problems such as phobias, depression, tic disorders, psychosomatic illnesses, near-sightedness,[5] allergy, common cold,[6] and learning disorders.[7][8]\n",
      "\n",
      "There is no scientific evidence supporting the claims made by NLP advocates and it has been discredited as a pseudoscience.[9][10][11]\n",
      "\n",
      "Scientific reviews state that NLP is based on outdated metaphors of how the brain works that are inconsistent with current neurological theory and contain numerous factual errors.[12][13]\n",
      "\n",
      "Reviews also found that all of the supportive research on NLP contained significant methodological flaws and that there were three times as many studies of a much higher quality that failed to reproduce the \"extraordinary claims\" made by Bandler, Grinder, and other NLP practitioners.[10][11]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to clean unneeded markings\n",
    "import re\n",
    "\n",
    "# data_clean = re.sub(\"\\[.+\\]\",\"\",data) #remove [NUM] tags # . means everything between first . and second .\n",
    "data_clean = re.sub(\"\\[[0-9]+\\]\",\"\",data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can still see unneeded new-line (\\n) characters, but the tokenizer will take care of those"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\arthu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\arthu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\arthu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\arthu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: click in c:\\users\\arthu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\arthu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm->nltk) (0.4.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'c:\\users\\arthu\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\arthu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Neuro-linguistic',\n",
       " 'programming',\n",
       " '(',\n",
       " 'NLP',\n",
       " ')',\n",
       " 'is',\n",
       " 'a',\n",
       " 'pseudoscientific',\n",
       " 'approach',\n",
       " 'to']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "! pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "data_clean_word_tokenized = word_tokenize(data_clean)\n",
    "data_clean_word_tokenized[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Side note: in some nlp tasks, you need to preserve the sentence separation. In that case, you must first separate by sentence, and then separate these sentences into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Neuro-linguistic programming (NLP) is a pseudoscientific approach to communication, personal development, and psychotherapy created by Richard Bandler and John Grinder in California, United States in the 1970s.',\n",
       " \"NLP's creators claim there is a connection between neurological processes (neuro-), language (linguistic) and behavioral patterns learned through experience (programming), and that these can be changed to achieve specific goals in life.\"]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "data_clean_sent_tokenized = sent_tokenize(data_clean)\n",
    "data_clean_sent_tokenized[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Neuro-linguistic',\n",
       " 'programming',\n",
       " '(',\n",
       " 'NLP',\n",
       " ')',\n",
       " 'is',\n",
       " 'a',\n",
       " 'pseudoscientific',\n",
       " 'approach',\n",
       " 'to',\n",
       " 'communication',\n",
       " ',',\n",
       " 'personal',\n",
       " 'development',\n",
       " ',',\n",
       " 'and',\n",
       " 'psychotherapy',\n",
       " 'created',\n",
       " 'by',\n",
       " 'Richard',\n",
       " 'Bandler',\n",
       " 'and',\n",
       " 'John',\n",
       " 'Grinder',\n",
       " 'in',\n",
       " 'California',\n",
       " ',',\n",
       " 'United',\n",
       " 'States',\n",
       " 'in',\n",
       " 'the',\n",
       " '1970s',\n",
       " '.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean_word_sent_tokenized = [word_tokenize(sentence) for sentence in data_clean_sent_tokenized]\n",
    "data_clean_word_sent_tokenized[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove punctuations with lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'few',\n",
       " 'words',\n",
       " 'about',\n",
       " 'dostoevsky',\n",
       " 'himself',\n",
       " 'may',\n",
       " 'help',\n",
       " 'the',\n",
       " 'english']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean_word_tokenized = [word.lower() for word in data_clean_word_tokenized if word.isalpha()]\n",
    "data_clean_word_tokenized[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['words',\n",
       " 'dostoevsky',\n",
       " 'may',\n",
       " 'help',\n",
       " 'english',\n",
       " 'reader',\n",
       " 'understand',\n",
       " 'work',\n",
       " 'dostoevsky',\n",
       " 'son']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "data_clean_word_tokenized = [word for word in data_clean_word_tokenized if not word in stopwords.words('english')]\n",
    "data_clean_word_tokenized[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization / POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word',\n",
       " 'dostoevsky',\n",
       " 'may',\n",
       " 'help',\n",
       " 'english',\n",
       " 'reader',\n",
       " 'understand',\n",
       " 'work',\n",
       " 'dostoevsky',\n",
       " 'son']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet') \n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    return ''\n",
    "    \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "data_clean_word_lemmatized = []\n",
    "\n",
    "for i, word in enumerate(data_clean_word_tokenized):\n",
    "    pos = get_wordnet_pos(pos_tag([word])[0][1])\n",
    "    if pos != '':\n",
    "        data_clean_word_lemmatized.append(lemmatizer.lemmatize(word, pos))\n",
    "    else:\n",
    "        data_clean_word_lemmatized.append(word)\n",
    "\n",
    "data_clean_word_lemmatized[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we have a dataset of pre-processed words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\arthu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\arthu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\arthu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\arthu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import string\n",
    "import random\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize, pos_tag\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize and perform lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean(text):\n",
    "    output = []\n",
    "    # tokenize\n",
    "    text = word_tokenize(text)\n",
    "    # only alphabets and numerics and lower case\n",
    "    text = [word.lower() for word in text if word.isalpha()]\n",
    "    # apply lemmatization\n",
    "    for i, word in enumerate(text):\n",
    "        pos = get_wordnet_pos(pos_tag([word])[0][1])\n",
    "        if pos != '':\n",
    "            output.append(lemmatizer.lemmatize(word, pos))\n",
    "        else:\n",
    "            output.append(word)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unnecessary characters, perform regex parsing\n",
    "def filter(text):\n",
    "    # remove punctuation and special characters\n",
    "    text = re.sub(\"\\[.,\\/#!$%\\^&\\*;:{}=\\-_`~()]\",\"\",text) \n",
    "    # replace newline with space\n",
    "    text = re.sub(\"[\\\\t\\\\n\\\\r]+\",\" \",text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions from the created 3-grams\n",
    "def predict(model, user_input):\n",
    "    print(\"Filtering user input...\")\n",
    "    text = filter(user_input)\n",
    "    print(\"Cleaning user input...\")\n",
    "    words = clean(text)\n",
    "    n = len(words)\n",
    "    preds = model[(words[n-3],words[n-2],words[n-1])].most_common(5)\n",
    "    print(preds)\n",
    "    print(text + \" \" + str(preds[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a language model using a dictionary, trigrams\n",
    "def n_gram_model(list_of_tokenized_text):\n",
    "    # a nifty tool to help us create ngrams. Here, ztri-grams\n",
    "    fourgrams = list(nltk.ngrams(list_of_tokenized_text, 4, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))\n",
    "    # now using nltk to get trigram frequency\n",
    "    a,b,c,d = list(zip(*fourgrams))\n",
    "    fourgrams = list(zip(a,b,c))\n",
    "    return nltk.ConditionalFreqDist(list(zip(fourgrams, d)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering corpus...\n",
      "Cleaning corpus...\n"
     ]
    }
   ],
   "source": [
    "# pre-process text data\n",
    " \n",
    "file = open('n-gram-data.txt', 'r')\n",
    "    \n",
    "text = \"\"\n",
    "while True:\n",
    "    line = file.readline()\n",
    "    text += line\n",
    "    if not line:\n",
    "        break\n",
    "\n",
    "# pre-process text\n",
    "print(\"Filtering corpus...\")\n",
    "text = filter(text)\n",
    "print(\"Cleaning corpus...\")\n",
    "words = clean(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A few words about Dostoevsky himself may help the English reader to understand his work.',\n",
       " 'Dostoevsky was the son of a doctor.',\n",
       " 'His parents were very hard- working and deeply religious people, but so poor that they lived with their five children in only two rooms.',\n",
       " 'The father and mother spent their evenings in reading aloud to their children, generally from books of a serious character.',\n",
       " 'Though always sickly and delicate Dostoevsky came out third in the final examination of the Petersburg school of Engineering.',\n",
       " 'There he had already begun his first work, \"Poor Folk.\"',\n",
       " 'This story was published by the poet Nekrassov in his review and was received with acclamations.',\n",
       " 'The shy, unknown youth found himself instantly something of a celebrity.',\n",
       " 'A brilliant and successful career seemed to open before him, but those hopes were soon dashed.',\n",
       " 'In 1849 he was arrested.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text_clean_sent_tokenized = sent_tokenize(text)\n",
    "text_clean_sent_tokenized[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making model...\n",
      "Enter a phrase: \n",
      "Filtering user input...\n",
      "Cleaning user input...\n",
      "[('and', 1)]\n",
      "Though always sickly and\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    # make language model\n",
    "    print(\"Making model...\")\n",
    "    model = n_gram_model(words)\n",
    "\n",
    "    print(\"Enter a phrase: \")\n",
    "    user_input = input()\n",
    "    predict(model, user_input)\n",
    "    \n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
