{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFC-H6caEaO2"
   },
   "source": [
    "We'll be using the Stanford AI Large Movie Review Dataset from https://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WpcCJt9UEKFC"
   },
   "source": [
    "First, Connect our google drive, or upload the dataset directly to the google colab session files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "Iboa6wsm0H-F",
    "outputId": "8d54bef7-b9d9-4a22-b18e-63bea30ec3e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3z2zOdgdEJbw"
   },
   "source": [
    "Extract the dataset to our working directory, or to your google colab folder if it is connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0A8sirRP_zT4"
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "tar = tarfile.open(\"./aclImdb_v1.tar.gz\")\n",
    "tar.extractall()\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UftUB77HXoO"
   },
   "source": [
    "# Reading and preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HyGrPKW-HXH3"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from random import shuffle\n",
    "from tqdm import tqdm\n",
    "def pre_process_data(filepath):\n",
    "\n",
    "\tpositive_path = os.path.join(filepath, 'pos')\n",
    "\tnegative_path = os.path.join(filepath, 'neg')\n",
    "\tpos_label = 1\n",
    "\tneg_label = 0\n",
    "\tdataset = []\n",
    "\n",
    "  # for the sake of memory limitations, we'll limit ourselves with only 5000 negative and 5000 positive samples \n",
    "\tfiles_count = 0\n",
    "\tfiles_limit = 5000\n",
    "\tfor filename in tqdm(glob.glob(os.path.join(positive_path, '*.txt'))):\n",
    "\t\twith open(filename, 'r') as f:\n",
    "\t\t\tdataset.append((pos_label, f.read()))\n",
    "\t\tfiles_count += 1\n",
    "\t\tif files_count > files_limit:\n",
    " \t\t\tbreak\n",
    "      \n",
    "\tfiles_count = 0\n",
    "\tfor filename in tqdm(glob.glob(os.path.join(negative_path, '*.txt'))):\n",
    "\t\twith open(filename, 'r') as f:\n",
    "\t\t\tdataset.append((neg_label, f.read()))\n",
    "\t\tfiles_count += 1\n",
    "\t\tif files_count > files_limit:\n",
    "\t\t\tbreak\n",
    "\n",
    "\tshuffle(dataset)\n",
    "\n",
    "\treturn dataset[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "Scz9S3weHe9D",
    "outputId": "958b00c3-b5e1-4ffa-d8ca-224ab0493c83"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████▊                      | 5000/12500 [00:13<00:19, 383.21it/s]\n",
      " 40%|██████████████▊                      | 5000/12500 [00:13<00:20, 373.12it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = pre_process_data('aclImdb/train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IowPEwfAnkEk"
   },
   "source": [
    "# Tokenizing and Vectorizing the data with word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " 'Made only ten years after the actual events, and set in the Bunker under the Reichstag, Pabst\\'s film is wholly gripping. It reeks of sulfurous death awaiting the perpetrators of world war. Haven\\'t seen this in over three decades, but it remains strong in my visual and emotional memory. The characters seem to be waiting to be walled up in their cave. Searing bit of dialog between two Generals: \"Does God exist?\" \"If He did, we wouldn\\'t.\" Shame this is not more readily available for exhibition or purchase because it would be interesting to view and compare this film with the documentary about Traudl Junge, \"Im Toten Winkel\" {aka \"Blind Spot: Hitler\\'s Secretary\") and \"Downfall\" with Bruno Ganz.')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qBbqICdQwi_H"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "eyAVKvV5FREw"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('/Users/khodor/Documents/Efrei/Courses/2021-2022/S8/NLP Course/exercizes/6-NN/GoogleNews-vectors-negative300-SLIM.bin.gz', binary=True)\n",
    "\n",
    "#'./drive/MyDrive/Colab Notebooks/GoogleNews-vectors-negative300-SLIM.bin.gz'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2zY1CgXwE0zi"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "hri3VAtzwr7N"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_vectorize(dataset):\n",
    "\tprint('tokenizing and vectorizing')\n",
    "\ttokenizer = TreebankWordTokenizer()\n",
    "\tvectorized_data = []\n",
    "\texpected = []\n",
    "\tfor sample in tqdm(dataset):\n",
    "\t\ttokens = tokenizer.tokenize(sample[1])\n",
    "\t\tsample_vecs = []\n",
    "\t\tfor token in tokens:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tsample_vecs.append(w2v[token])\n",
    "\t\t\texcept KeyError:\n",
    "\t\t\t\tpass # No matching token in the Google w2v vocab\n",
    "\t\t\n",
    "\t\tvectorized_data.append(sample_vecs)\n",
    "\t\n",
    "\treturn vectorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "mPqLGZD9wYot"
   },
   "outputs": [],
   "source": [
    "def collect_expected(dataset):\n",
    "    expected = []\n",
    "    for sample in tqdm(dataset):\n",
    "        expected.append(sample[0])\n",
    "    return expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "ST0QlGDHwbdU",
    "outputId": "d308e3b6-5da4-4090-c9d8-c7d70066d963"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing and vectorizing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 10002/10002 [00:08<00:00, 1236.08it/s]\n",
      "100%|████████████████████████████████| 10002/10002 [00:00<00:00, 3292373.93it/s]\n"
     ]
    }
   ],
   "source": [
    "vectorized_data = tokenize_and_vectorize(dataset)\n",
    "expected = collect_expected(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ZP30C7TuBImT"
   },
   "outputs": [],
   "source": [
    "# delete the word2vec model to save memory\n",
    "# w2v = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "nnB35xuhwcnp"
   },
   "outputs": [],
   "source": [
    "split_point = int(len(vectorized_data)*.8)\n",
    "x_train = vectorized_data[:split_point]\n",
    "y_train = expected[:split_point]\n",
    "x_test = vectorized_data[split_point:]\n",
    "y_test = expected[split_point:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfNmwJZdnuQS"
   },
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vNdlDgLantmD"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence # A helper module to handle padding input\n",
    "from keras.models import Sequential # The base Keras neural network model\n",
    "from keras.layers import Dense, Dropout, Activation #The layer objects you’ll pile into the model\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D # Your convolution layer, and pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "9PpB4dtRwsYw"
   },
   "outputs": [],
   "source": [
    "maxlen = 200\n",
    "batch_size = 32 # How many samples to show the net before backpropagating the error and updating the weights\n",
    "embedding_dims = 300 # Length of the token vectors you’ll create for passing into the convnet\n",
    "filters = 100 # Number of filters you’ll train\n",
    "kernel_size = 3 # The width of the filters; actual filters will each be a matrix\n",
    "                # of weights of size: embedding_dims x kernel_size, or 50 x 3 in your case\n",
    "hidden_dims = 100 # Number of neurons in the plain feedforward net at the end of the chain\n",
    "epochs = 2 # Number of times you’ll pass the entire training dataset through the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7NATZQIoRNv"
   },
   "source": [
    "### For a given dataset pad with zero vectors or truncate to maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "6-JvBbEYwtvJ"
   },
   "outputs": [],
   "source": [
    "def pad_trunc(data, maxlen):\n",
    "\tnew_data = []\n",
    "\t# Create a vector of 0s the length of our word vectors\n",
    "\tzero_vector = []\n",
    "\tfor _ in range(len(data[0][0])):\n",
    "\t\tzero_vector.append(0.0)\n",
    "\t\n",
    "\tfor sample in tqdm(data):\n",
    "\t\tif len(sample) > maxlen:\n",
    "\t\t\ttemp = sample[:maxlen]\n",
    "\t\telif len(sample) < maxlen:\n",
    "\t\t\ttemp = sample\n",
    "\t\t\t# Append the appropriate number 0 vectors to the list\n",
    "\t\t\tadditional_elems = maxlen - len(sample)\n",
    "\t\t\tfor _ in range(additional_elems):\n",
    "\t\t\t\ttemp.append(zero_vector)\n",
    "\t\telse:\n",
    "\t\t\ttemp = sample\n",
    "\t\tnew_data.append(temp)\n",
    "\treturn new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "cL5trcO_w66I",
    "outputId": "d120dd47-2bdc-4985-942e-8639919b0ce9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8001/8001 [00:00<00:00, 145174.26it/s]\n",
      "100%|██████████| 2001/2001 [00:00<00:00, 106463.06it/s]\n"
     ]
    }
   ],
   "source": [
    "x_train = pad_trunc(x_train, maxlen)\n",
    "x_test = pad_trunc(x_test, maxlen)\n",
    "x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims))\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32ojBtHCxzfX"
   },
   "source": [
    "## Model Layers Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "isUoZkbNw9yO",
    "outputId": "89992b00-c219-4c3a-cf2f-a2bdddbfb5b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D( # the first layer you add is the convolutional layer\n",
    "  filters,\n",
    "  kernel_size,\n",
    "  padding='valid', # assume that it's ok for the output to be of smaller domentions than the input \n",
    "  activation='relu',\n",
    "  strides=1,\n",
    "  input_shape=(maxlen, embedding_dims))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Z20Rx7e-3NwZ"
   },
   "outputs": [],
   "source": [
    "model.add(GlobalMaxPooling1D()) # Max Pooling layer.\n",
    "                                # Instead of taking the max of a small subsection of each filter’s output,\n",
    "                                # you’re taking the max of the entire output for that filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Dj3xhdkyx28A"
   },
   "outputs": [],
   "source": [
    "model.add(Dense(hidden_dims)) # standard feedforward network\n",
    "model.add(Dropout(0.2)) # only 80% of the embedding data, randomly chosen for\n",
    "                        # each training sample, will pass into the next layer as it is. The rest will go in as 0s.\n",
    "model.add(Activation('relu')) # use the Rectified Linear Units activation (relu) on the output end of each neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "beErZ2alyFXo"
   },
   "outputs": [],
   "source": [
    "# Here is the actual classifier\n",
    "# a neuron that fires based on the sigmoid activation function; it gives a value between 0 and 1.\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZJFsMgUx-Zo"
   },
   "source": [
    "## Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "dCdwtS1ByHZE"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', #The loss function is what the network will try to minimize\n",
    "  optimizer='adam', # an optimization algorithm to minimizing the loss function\n",
    "  metrics=['accuracy'] # based on what will we evaluate the performance of our model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3HFvo6biypFK"
   },
   "source": [
    "## Start the Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "qClmYqbwyM56",
    "outputId": "966a6b06-3771-4d8e-98c2-ae48922d1052"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 0.2588 - accuracy: 0.8935 - val_loss: 0.3512 - val_accuracy: 0.8486\n",
      "Epoch 2/2\n",
      "251/251 [==============================] - 2s 7ms/step - loss: 0.1831 - accuracy: 0.9303 - val_loss: 0.4117 - val_accuracy: 0.8311\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9b9ff9a990>"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "  batch_size=batch_size,\n",
    "  epochs=epochs,\n",
    "  validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EiHx5tq9ytL3"
   },
   "source": [
    "## Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "sbQfq2ssDVCn",
    "outputId": "7cbfb60d-8db9-493d-8ba1-6dc9721d531f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 3266.59it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1803.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing and vectorizing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1]], dtype=int32)"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smpl =  tokenize_and_vectorize([(1, \"The sugar is sweet\")])\n",
    "smpl = pad_trunc(smpl, maxlen)\n",
    "test_vec = np.reshape(smpl, (len(smpl), maxlen, embedding_dims))\n",
    "model.predict_classes(test_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1cVXBb1yw_w"
   },
   "source": [
    "## Save and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IO_aCCfxy-cA"
   },
   "outputs": [],
   "source": [
    "# save the structure of the created model\n",
    "model_structure = model.to_json()\n",
    "with open(\"cnn_model.json\", \"w\") as json_file:\n",
    "  json_file.write(model_structure)\n",
    "\n",
    "# save the trained weights of the model\n",
    "model.save_weights(\"cnn_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UZTqrgclywnh"
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "# load the structure of a previously created model\n",
    "with open(\"cnn_model.json\", \"r\") as json_file:\n",
    "  json_string = json_file.read()\n",
    "model = model_from_json(json_string)\n",
    "\n",
    "# load the previously trained weights\n",
    "model.load_weights('cnn_weights.h5')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Chapter 7: CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
